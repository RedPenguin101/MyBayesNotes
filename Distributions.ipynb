{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = [3, 3]\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All distributions share the following common attributes:\n",
    "* They have a denotation $X \\sim  \\text{Name(statistical parameters)}$\n",
    "* They have a function $f(x|\\text{sp})$ which gets you from the $x$ value to the probability of that $x$ value, i.e. $P(X=x)$\n",
    "* They have an expected value $E[X]$, aka **mean**. It's the weighted average of all values $X$ can take, with the weighting done by the probability of the the values. \n",
    "* They have a variance $\\text{Var}[X]$. This is a measure of how spread out its values are. In words it can be described as  how far you expect values of X to be from the mean squared, or $E[(X-\\mu)^2]$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$X$ here is a **random variable**, and $x$ denotes the set of possible values $X$ can take.\n",
    "\n",
    "Put another way, $X$ is an event or measurement, the outcome or value of which we aren't certain of. \n",
    "\n",
    "So you can say the probability that $X$ is a particular $x$ is based on the probability function of $X$ and the various statistical parameters which describe the distribution.\n",
    "\n",
    "$$P(X=x|\\text{sp}) = f(x|\\text{sp})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mathematical vs model approximation distributions\n",
    "\n",
    "You can think of a distribution in two ways. First there is the mathematical distributions. These are like the platonic ideals, like the straight line or perfect circle: They exist only in our heads, not in the real world.\n",
    "\n",
    "When we bring things into the real world we have to deal in approximations, like if you draw a circle on a piece of paper, that 'approximates' the platonic form of a circle. If we have a random event in the real world, that can be similar to one of the mythical distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discrete and Continuous Distributions\n",
    "Distributions can be divided into discrete or continuous distributions. This describes the nature of the input $X$. \n",
    "\n",
    "If the input is an event, in the form of the number of successes, counting the number of occurances, or events where the outcome is a set of integers, then these might be approximated with a discrete distribution. Discrete distributions include:\n",
    "\n",
    "* Bernoulli (for two possible outcomes of a particular event, success or failure)\n",
    "* Binomial (n repeated Bernoulli trials)\n",
    "* Geometric (number of Bernoulli trials needed until you get the first success)\n",
    "* Multinomial (Like a binomial where there are more than two outcomes) \n",
    "* Poisson (Use it when you are counting the occurance of events, either in time or space)\n",
    "\n",
    "If the input is continuous (can be anything on a real number scale: heights etc.) then a continuous distribution is usually a better approximation. Note that the *result* of the probability function isn't necessarily continuous: see uniform distribution below.\n",
    "\n",
    "An important difference between calculating probabilities in continuous distributions is that you do it *between two values* or over an interval - the probability that x is a specific real number is 0. \n",
    "\n",
    "* Uniform - the probability is evenly distributed over an interval (i.e. is equally likely to take any value in that interval) but is nil outside it.\n",
    "* Exponential - describes events occuring at a given rate.  \n",
    "* Normal / Gaussian\n",
    "* T Distribution\n",
    "* Beta \n",
    "* Gamma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probability Density or Mass Function (PDF/PMF)\n",
    "\n",
    "Each distribution has (or more accrurately is defined by) a function which translates possible observations $x$ of an uncertain variable $X$ with the probability of getting that observation. The function is a function of $x$ and the statistical parameters of the distribution. \n",
    "\n",
    "$$P(X=x) = f(x \\mid \\text{statistical parameters})$$\n",
    "\n",
    "For discrete distributions, where $x$ can be integer values, this function is referred to as the Probability Mass Function, PMF, and for continouous distributions as the Probability Density Function.\n",
    "\n",
    "Using this function, it's easy to figure out $P(X=x)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cumulative Density Function\n",
    "\n",
    "If the PDF is the probability that $X=x$, the CDF is the probability that $X$ will fall between two values.\n",
    "\n",
    "For discretes this is simple to conceptualise: It's the sum of each probability in the range you are looking at:\n",
    "\n",
    "$$P(a \\le X \\le b) = F(x) = \\sum_{i=a}^{b} f(x)$$\n",
    "\n",
    "For cumulative it's a big tricker. The best way to think about it is to picture the PDF as a graph of $x$ and $f(x)$. The probability that $x$ falls between two values is the area under the PDF line between two points, or in other words, the integral:\n",
    "\n",
    "$$P(a \\le X \\le b) = F(x) = \\int_{a}^{b} f(x) dx$$\n",
    "\n",
    "The CDF that $X$ is any value in the population will always be 1, i.e.\n",
    "\n",
    "$$P(1 \\le X \\le n) = 1$$\n",
    "\n",
    "$$P(-\\infty \\le X \\le \\infty) = \\int_{-\\infty}^{\\infty} f(x) dx = 1$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expectation\n",
    "\n",
    "The expected value, $E[X]$, or **mean**, of a distribution is the probability weighted average. For a discrete distribution you take all the possible values of X, multiply them by P(X), and sum up the results.\n",
    "\n",
    "$$E[X] = \\sum_{i=1}^n x_i P(X_i = x_i) = \\sum_{i=1}^n x_i f(x_i)$$\n",
    "\n",
    "$$E[X] = \\int_{-\\infty}^\\infty x f(x) dx$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
